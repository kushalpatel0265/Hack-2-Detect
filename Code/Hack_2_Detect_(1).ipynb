{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "collapsed_sections": [
        "7GDj_4lNTpyg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset Preparation"
      ],
      "metadata": {
        "id": "7GDj_4lNTpyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "mZB1JheV3IZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBYAeXy42q8b",
        "outputId": "4c7b3f34-480f-47ed-c801-c3d97ee21169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Person Detection"
      ],
      "metadata": {
        "id": "K8EFj-7UTuNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install YOLOv8 (ultralytics package)"
      ],
      "metadata": {
        "id": "BeNinrWs3nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWuRxdvr3JJP",
        "outputId": "ee13299c-303d-4646-89d2-52f3286146a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.75-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.75-py3-none-any.whl (914 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.75 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "3VDb5SHG4Lep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc"
      ],
      "metadata": {
        "id": "aLTYXGFk3sJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bef145-a6c8-48ac-8271-3beeff39467f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the YOLOv8 model (nano version for speed; adjust as needed)"
      ],
      "metadata": {
        "id": "xd0SmR1N4Txe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n.pt\")  # This downloads the model if not already present"
      ],
      "metadata": {
        "id": "9HBSJ-734Ood",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf10788-909a-47b2-bc95-2485485adadb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 48.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define paths for each camera's images"
      ],
      "metadata": {
        "id": "YPbhnr1j4ZSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the base_path to match the location of your dataset on Drive.\n",
        "base_path = \"/content/drive/MyDrive/dataset/Wildtrack/Image_subsets\"\n",
        "camera_dirs = {\n",
        "    \"Camera_1\": os.path.join(base_path, \"C1\"),\n",
        "    \"Camera_2\": os.path.join(base_path, \"C2\"),\n",
        "    \"Camera_3\": os.path.join(base_path, \"C3\"),\n",
        "    \"Camera_4\": os.path.join(base_path, \"C4\"),\n",
        "    \"Camera_5\": os.path.join(base_path, \"C5\"),\n",
        "    \"Camera_6\": os.path.join(base_path, \"C6\"),\n",
        "    \"Camera_7\": os.path.join(base_path, \"C7\")\n",
        "}"
      ],
      "metadata": {
        "id": "2Z2Oisqx4Wnr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directory to save intermediate detection results"
      ],
      "metadata": {
        "id": "kjMr5qz24y4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/MyDrive\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "rDQjw1Mm4sXA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(output_dir):\n",
        "    print(\"Creating output directory at:\", output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "else:\n",
        "    print(\"Output directory exists:\", output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmRp3RZHPNHl",
        "outputId": "913e9c9f-b38c-445d-be0b-cff1cb154a14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory exists: /content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set detection confidence threshold"
      ],
      "metadata": {
        "id": "uMN4u28HBpAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_thresh = 0.5"
      ],
      "metadata": {
        "id": "s8VJauIgBtGM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process images for each camera folder"
      ],
      "metadata": {
        "id": "5ek2VqS85i-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for camera_id, camera_path in camera_dirs.items():\n",
        "    print(f\"Processing images for {camera_id} in {camera_path}\")\n",
        "    detections_storage = {}\n",
        "\n",
        "    # List and sort image files (supports .jpg, .jpeg, .png)\n",
        "    image_files = sorted([f for f in os.listdir(camera_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "    batch_size = 15\n",
        "    for i in range(0, len(image_files), batch_size):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        for image_name in batch_files:\n",
        "            image_path = os.path.join(camera_path, image_name)\n",
        "            frame = cv2.imread(image_path)\n",
        "            if frame is None:\n",
        "                continue\n",
        "\n",
        "            # Run YOLOv8 on the image\n",
        "            results = model(frame, verbose=False)\n",
        "            person_detections = []\n",
        "            for box in results[0].boxes.data.cpu().numpy():\n",
        "                # Each detection box: [x1, y1, x2, y2, confidence, class]\n",
        "                x1, y1, x2, y2, conf, cls = box\n",
        "                # Filter for 'person' detections (COCO class id 0)\n",
        "                if int(cls) == 0 and conf >= conf_thresh:\n",
        "                    x = int(x1)\n",
        "                    y = int(y1)\n",
        "                    w = int(x2 - x1)\n",
        "                    h = int(y2 - y1)\n",
        "                    person_detections.append((x, y, w, h, float(conf)))\n",
        "            detections_storage[image_name] = person_detections\n",
        "\n",
        "            # (Optional) Visualize only the first image in each batch to monitor progress\n",
        "            if batch_files.index(image_name) == 0:\n",
        "                for (x, y, w, h, conf) in person_detections:\n",
        "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, f\"{conf:.2f}\", (x, y-10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "                cv2_imshow(frame)\n",
        "                cv2.waitKey(10)\n",
        "\n",
        "        # Save intermediate results to disk after processing each batch\n",
        "        intermediate_file = os.path.join(output_dir, f\"{camera_id}_detections_{i}_{i+batch_size}.json\")\n",
        "        with open(intermediate_file, 'w') as f:\n",
        "            json.dump(detections_storage, f)\n",
        "        print(f\"Saved detections for images {i} to {i+batch_size} of {camera_id}\")\n",
        "\n",
        "        # Clear the storage and output to prevent disconnection\n",
        "        detections_storage.clear()\n",
        "        gc.collect()\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    print(f\"Finished processing {camera_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "puZVJASb43fS",
        "outputId": "168006f8-d01c-458d-95cd-4a0200355314"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing Camera_3\n",
            "Processing images for Camera_4 in /content/drive/MyDrive/dataset/Wildtrack/Image_subsets/C4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/Wildtrack/Image_subsets/C4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1288ab907dc3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# List and sort image files (supports .jpg, .jpeg, .png)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamera_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/Wildtrack/Image_subsets/C4'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Parsing Annotations & Recovering 3D Coordinates"
      ],
      "metadata": {
        "id": "NitHk3r3T2Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "fmK824T9WsFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute 3D coordinates given a positionID\n",
        "\n",
        "def compute_3d_coordinates(position_id):\n",
        "    X = -3.0 + 0.025 * (position_id % 480)\n",
        "    Y = -9.0 + 0.025 * (position_id // 480)\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "RMJed5oPW4la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load Annotations & Compute 3D Coordinates"
      ],
      "metadata": {
        "id": "nq2XMLwuYE4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the annotations folder (update with your Drive path)\n",
        "annotations_dir = \"/content/drive/MyDrive/Wildtrack/annotations_positions\"\n",
        "\n",
        "# Dictionary to store processed annotations (keyed by JSON filename)\n",
        "annotations = {}\n",
        "\n",
        "# Loop through each JSON file in the annotations folder\n",
        "for filename in sorted(os.listdir(annotations_dir)):\n",
        "    if filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(annotations_dir, filename)\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # List to store all annotations for this frame/file\n",
        "        frame_annotations = []\n",
        "        for ann in data:\n",
        "            # Extract metadata (adjust key names if necessary)\n",
        "            position_id = ann.get(\"positionID\")\n",
        "            rectangle_id = ann.get(\"rectangleID\")\n",
        "            camera_id = ann.get(\"cameraID\")\n",
        "\n",
        "            # Compute 3D coordinates using the provided formulas\n",
        "            X, Y = compute_3d_coordinates(position_id)\n",
        "\n",
        "            # Store the annotation information\n",
        "            annotation_info = {\n",
        "                \"positionID\": position_id,\n",
        "                \"rectangleID\": rectangle_id,\n",
        "                \"cameraID\": camera_id,\n",
        "                \"X\": X,\n",
        "                \"Y\": Y\n",
        "            }\n",
        "            frame_annotations.append(annotation_info)\n",
        "\n",
        "        # Use the filename as the key (or extract a frame number if needed)\n",
        "        annotations[filename] = frame_annotations"
      ],
      "metadata": {
        "id": "r4iE1qd5X3SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Parse rectangles.pom to Build a Mapping"
      ],
      "metadata": {
        "id": "ZI7lDa_8YkTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the rectangles.pom file (update with your Drive path)\n",
        "rectangles_pom_file = \"/content/drive/MyDrive/Wildtrack/rectangles.pom\"\n",
        "\n",
        "# Dictionary to store rectangle mapping: key=(cameraID, rectangleID), value=(x, y, w, h)\n",
        "rect_map = {}\n",
        "with open(rectangles_pom_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"Rectangle\"):\n",
        "            # Expected format: \"Rectangle ID=178 Cam=1 X=531 Y=192 W=32 H=88\"\n",
        "            m = re.search(r\"Rectangle ID=(\\d+)\\s+Cam=(\\d+)\\s+X=([\\d\\.]+)\\s+Y=([\\d\\.]+)\\s+W=([\\d\\.]+)\\s+H=([\\d\\.]+)\", line)\n",
        "            if m:\n",
        "                rect_id = int(m.group(1))\n",
        "                cam_id = int(m.group(2))\n",
        "                x_val = float(m.group(3))\n",
        "                y_val = float(m.group(4))\n",
        "                w_val = float(m.group(5))\n",
        "                h_val = float(m.group(6))\n",
        "                rect_map[(cam_id, rect_id)] = (x_val, y_val, w_val, h_val)"
      ],
      "metadata": {
        "id": "urgZmtxHX6AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Match Annotations with 2D Detections Using the Rectangle Mapping"
      ],
      "metadata": {
        "id": "GwKI3MaYZF6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for frame_file, ann_list in annotations.items():\n",
        "    for ann in ann_list:\n",
        "        cam_id = ann.get(\"cameraID\")\n",
        "        rect_id = ann.get(\"rectangleID\")\n",
        "        # Look up the 2D bounding box from the rectangles mapping\n",
        "        bbox_2d = rect_map.get((cam_id, rect_id))\n",
        "        ann[\"bbox_2d\"] = bbox_2d  # This will be None if no match is found"
      ],
      "metadata": {
        "id": "S1uJiwYFY-q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Sample Output for Verification"
      ],
      "metadata": {
        "id": "1oq1yNf2ZLaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for frame_file, ann_list in annotations.items():\n",
        "    print(\"Frame:\", frame_file)\n",
        "    for ann in ann_list:\n",
        "        print(ann)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "cyKZS2b0ZJNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# -------------------------------\n",
        "# Helper Function: Compute 3D Coordinates\n",
        "# -------------------------------\n",
        "def compute_3d_coordinates(position_id):\n",
        "    # Formulas:\n",
        "    #   X = -3.0 + 0.025 * (positionID % 480)\n",
        "    #   Y = -9.0 + 0.025 * (positionID // 480)\n",
        "    X = -3.0 + 0.025 * (position_id % 480)\n",
        "    Y = -9.0 + 0.025 * (position_id // 480)\n",
        "    return X, Y\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Load Annotations & Compute 3D Coordinates\n",
        "# -------------------------------\n",
        "annotations_dir = \"/content/drive/MyDrive/Wildtrack/annotations_positions\"\n",
        "output_annotations = {}  # Will store: {frame_filename: [annotation, ...], ...}\n",
        "\n",
        "for filename in sorted(os.listdir(annotations_dir)):\n",
        "    if filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(annotations_dir, filename)\n",
        "        with open(filepath, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        frame_annotations = []\n",
        "        for ann in data:\n",
        "            # Extract metadata using the available keys\n",
        "            personID = ann.get(\"personID\")\n",
        "            positionID = ann.get(\"positionID\")\n",
        "            views = ann.get(\"views\", [])\n",
        "\n",
        "            # Skip if no positionID\n",
        "            if positionID is None:\n",
        "                continue\n",
        "\n",
        "            # Compute 3D coordinates\n",
        "            X, Y = compute_3d_coordinates(positionID)\n",
        "\n",
        "            # Build a list of view annotations (each view provides 2D bounding box info)\n",
        "            view_annotations = []\n",
        "            for v in views:\n",
        "                viewNum = v.get(\"viewNum\")  # This will be used as the camera identifier\n",
        "                xmin = v.get(\"xmin\")\n",
        "                xmax = v.get(\"xmax\")\n",
        "                ymin = v.get(\"ymin\")\n",
        "                ymax = v.get(\"ymax\")\n",
        "                view_info = {\n",
        "                    \"viewNum\": viewNum,\n",
        "                    \"xmin\": xmin,\n",
        "                    \"xmax\": xmax,\n",
        "                    \"ymin\": ymin,\n",
        "                    \"ymax\": ymax,\n",
        "                    \"bbox_2d\": None  # Placeholder; will be computed in optional matching below\n",
        "                }\n",
        "                view_annotations.append(view_info)\n",
        "\n",
        "            annotation_info = {\n",
        "                \"personID\": personID,\n",
        "                \"positionID\": positionID,\n",
        "                \"X\": X,\n",
        "                \"Y\": Y,\n",
        "                \"views\": view_annotations  # List of per-view annotations\n",
        "            }\n",
        "            frame_annotations.append(annotation_info)\n",
        "\n",
        "        output_annotations[filename] = frame_annotations\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Parse rectangles.pom to Build a Rectangle Mapping (Optional)\n",
        "# -------------------------------\n",
        "# This mapping is built for reference. Your annotations don't include rectangleID,\n",
        "# so we won't use it directly, but it's available for further association if needed.\n",
        "rectangles_pom_file = \"/content/drive/MyDrive/Wildtrack/rectangles.pom\"\n",
        "rect_map = {}  # Mapping: {(cameraID, rectangleID): (x, y, w, h)}\n",
        "\n",
        "with open(rectangles_pom_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"Rectangle\"):\n",
        "            # Expected format: \"Rectangle ID=178 Cam=1 X=531 Y=192 W=32 H=88\"\n",
        "            match = re.search(r\"Rectangle\\s+ID=(\\d+)\\s+Cam=(\\d+)\\s+X=([\\d\\.]+)\\s+Y=([\\d\\.]+)\\s+W=([\\d\\.]+)\\s+H=([\\d\\.]+)\", line)\n",
        "            if match:\n",
        "                rect_id = int(match.group(1))\n",
        "                cam_id = int(match.group(2))\n",
        "                x_val = float(match.group(3))\n",
        "                y_val = float(match.group(4))\n",
        "                w_val = float(match.group(5))\n",
        "                h_val = float(match.group(6))\n",
        "                rect_map[(cam_id, rect_id)] = (x_val, y_val, w_val, h_val)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Optional Matching Using \"viewNum\" from Annotations\n",
        "# -------------------------------\n",
        "# Since your annotations don't provide explicit rectangleID or cameraID,\n",
        "# we'll use the \"viewNum\" in each view as the camera identifier.\n",
        "# And we'll compute a 2D bounding box from the annotated xmin, xmax, ymin, ymax values.\n",
        "for frame_file, ann_list in output_annotations.items():\n",
        "    for ann in ann_list:\n",
        "        for view in ann.get(\"views\", []):\n",
        "            xmin = view.get(\"xmin\")\n",
        "            xmax = view.get(\"xmax\")\n",
        "            ymin = view.get(\"ymin\")\n",
        "            ymax = view.get(\"ymax\")\n",
        "            # Check if the values are valid (not -1)\n",
        "            if (xmin is not None and xmax is not None and ymin is not None and ymax is not None and\n",
        "                xmin != -1 and xmax != -1 and ymin != -1 and ymax != -1):\n",
        "                width = xmax - xmin\n",
        "                height = ymax - ymin\n",
        "                view[\"bbox_2d\"] = (xmin, ymin, width, height)\n",
        "            else:\n",
        "                view[\"bbox_2d\"] = None\n",
        "            # Optionally, set the annotation's cameraID if it's not set, using viewNum.\n",
        "            if ann.get(\"cameraID\") is None:\n",
        "                ann[\"cameraID\"] = view.get(\"viewNum\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Print Sample Output for Verification\n",
        "# -------------------------------\n",
        "for frame_file, ann_list in output_annotations.items():\n",
        "    print(\"Frame:\", frame_file)\n",
        "    for ann in ann_list:\n",
        "        print(ann)\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "hmOTMv0DfL3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import cv2\n",
        "\n",
        "# -------------------------------\n",
        "# Dummy Placeholder for DeepOcclusionRefiner\n",
        "# -------------------------------\n",
        "try:\n",
        "    from deep_occlusion import DeepOcclusionRefiner  # Production module\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Warning: deep_occlusion module not found. Using dummy occlusion refiner.\")\n",
        "    class DeepOcclusionRefiner:\n",
        "        def __init__(self, model_path, config_path):\n",
        "            self.model_path = model_path\n",
        "            self.config_path = config_path\n",
        "            print(f\"Initialized dummy DeepOcclusionRefiner with model: {model_path} and config: {config_path}\")\n",
        "        def refine_bbox(self, image, bbox):\n",
        "            # In production, apply occlusion handling here.\n",
        "            # For now, simply return the original bounding box.\n",
        "            return bbox\n",
        "\n",
        "# -------------------------------\n",
        "# Helper Function: Compute 3D Coordinates\n",
        "# -------------------------------\n",
        "def compute_3d_coordinates(position_id):\n",
        "    # Using formulas:\n",
        "    #   X = -3.0 + 0.025 * (positionID % 480)\n",
        "    #   Y = -9.0 + 0.025 * (positionID // 480)\n",
        "    X = -3.0 + 0.025 * (position_id % 480)\n",
        "    Y = -9.0 + 0.025 * (position_id // 480)\n",
        "    return X, Y\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Load Annotations & Compute 3D Coordinates\n",
        "# -------------------------------\n",
        "annotations_dir = \"/content/drive/MyDrive/Wildtrack/annotations_positions\"\n",
        "output_annotations = {}  # {frame_filename: [annotation, ...], ...}\n",
        "\n",
        "for filename in sorted(os.listdir(annotations_dir)):\n",
        "    if filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(annotations_dir, filename)\n",
        "        with open(filepath, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        frame_annotations = []\n",
        "        for ann in data:\n",
        "            # Extract available metadata from annotations\n",
        "            personID = ann.get(\"personID\")\n",
        "            positionID = ann.get(\"positionID\")\n",
        "            views = ann.get(\"views\", [])\n",
        "            if positionID is None:\n",
        "                continue  # Skip if no positionID\n",
        "\n",
        "            # Compute 3D coordinates from positionID\n",
        "            X, Y = compute_3d_coordinates(positionID)\n",
        "\n",
        "            # Process each view to extract the 2D bounding box\n",
        "            view_annotations = []\n",
        "            for v in views:\n",
        "                viewNum = v.get(\"viewNum\")  # Using this as camera identifier\n",
        "                xmin = v.get(\"xmin\")\n",
        "                xmax = v.get(\"xmax\")\n",
        "                ymin = v.get(\"ymin\")\n",
        "                ymax = v.get(\"ymax\")\n",
        "                bbox_2d = None\n",
        "                if xmin != -1 and xmax != -1 and ymin != -1 and ymax != -1:\n",
        "                    width = xmax - xmin\n",
        "                    height = ymax - ymin\n",
        "                    bbox_2d = (xmin, ymin, width, height)\n",
        "                view_annotations.append({\n",
        "                    \"viewNum\": viewNum,\n",
        "                    \"xmin\": xmin,\n",
        "                    \"xmax\": xmax,\n",
        "                    \"ymin\": ymin,\n",
        "                    \"ymax\": ymax,\n",
        "                    \"bbox_2d\": bbox_2d\n",
        "                })\n",
        "\n",
        "            annotation_info = {\n",
        "                \"personID\": personID,\n",
        "                \"positionID\": positionID,\n",
        "                \"X\": X,\n",
        "                \"Y\": Y,\n",
        "                \"views\": view_annotations\n",
        "            }\n",
        "            frame_annotations.append(annotation_info)\n",
        "\n",
        "        output_annotations[filename] = frame_annotations\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Parse rectangles.pom to Build a Rectangle Mapping (Optional)\n",
        "# -------------------------------\n",
        "rectangles_pom_file = \"/content/drive/MyDrive/Wildtrack/rectangles.pom\"\n",
        "rect_map = {}  # Mapping: {(cameraID, rectangleID): (x, y, w, h)}\n",
        "with open(rectangles_pom_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"Rectangle\"):\n",
        "            match = re.search(r\"Rectangle\\s+ID=(\\d+)\\s+Cam=(\\d+)\\s+X=([\\d\\.]+)\\s+Y=([\\d\\.]+)\\s+W=([\\d\\.]+)\\s+H=([\\d\\.]+)\", line)\n",
        "            if match:\n",
        "                rect_id = int(match.group(1))\n",
        "                cam_id = int(match.group(2))\n",
        "                x_val = float(match.group(3))\n",
        "                y_val = float(match.group(4))\n",
        "                w_val = float(match.group(5))\n",
        "                h_val = float(match.group(6))\n",
        "                rect_map[(cam_id, rect_id)] = (x_val, y_val, w_val, h_val)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: (Optional) Matching using \"viewNum\"\n",
        "# -------------------------------\n",
        "# In this implementation, each view's bbox_2d is directly computed from annotation.\n",
        "# Additional matching using rect_map can be implemented if needed.\n",
        "# For now, we simply assume the annotation's bbox_2d is our ground truth.\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Integrate Occlusion Handling using DeepOcclusion (Production-Level)\n",
        "# -------------------------------\n",
        "# Initialize the occlusion refiner with your production model and configuration.\n",
        "occlusion_refiner = DeepOcclusionRefiner(\n",
        "    model_path=\"/content/drive/MyDrive/deep_occlusion_model.pth\",\n",
        "    config_path=\"/content/drive/MyDrive/deep_occlusion_config.yaml\"\n",
        ")\n",
        "\n",
        "# For production, process each frame to refine each valid bounding box using the occlusion handler.\n",
        "# In a real project, you would load the correct image from the corresponding camera folder.\n",
        "# Here, we assume that the annotation filename \"00001020.json\" corresponds to an image \"00001020.jpg\".\n",
        "# Adjust 'base_images_dirs' to map viewNum to the correct folder.\n",
        "base_images_dirs = {\n",
        "    # Assuming viewNum 0 corresponds to camera folder \"C1\", viewNum 1 -> \"C2\", etc.\n",
        "    0: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C1\",\n",
        "    1: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C2\",\n",
        "    2: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C3\",\n",
        "    3: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C4\",\n",
        "    4: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C5\",\n",
        "    5: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C6\",\n",
        "    6: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C7\"\n",
        "}\n",
        "\n",
        "# Iterate over all annotations and refine bboxes for each view using occlusion handling.\n",
        "for frame_file, ann_list in output_annotations.items():\n",
        "    # Derive image filename from frame_file (e.g., \"00001020.json\" -> \"00001020.jpg\")\n",
        "    image_filename = frame_file.replace(\".json\", \".jpg\")\n",
        "    for ann in ann_list:\n",
        "        for view in ann.get(\"views\", []):\n",
        "            if view.get(\"bbox_2d\") is not None:\n",
        "                view_num = view.get(\"viewNum\")\n",
        "                # Get the base image directory for this viewNum\n",
        "                base_dir = base_images_dirs.get(view_num)\n",
        "                if base_dir is None:\n",
        "                    continue\n",
        "                image_path = os.path.join(base_dir, image_filename)\n",
        "                if not os.path.exists(image_path):\n",
        "                    continue\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is None:\n",
        "                    continue\n",
        "                # Refine the bounding box using the occlusion refiner\n",
        "                original_bbox = view[\"bbox_2d\"]\n",
        "                refined_bbox = occlusion_refiner.refine_bbox(image, original_bbox)\n",
        "                view[\"bbox_2d\"] = refined_bbox\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Save the Final Refined Annotations\n",
        "# -------------------------------\n",
        "final_output_file = \"/content/drive/MyDrive/Wildtrack/refined_annotations.json\"\n",
        "with open(final_output_file, \"w\") as f:\n",
        "    json.dump(output_annotations, f, indent=2)\n",
        "\n",
        "# -------------------------------\n",
        "# (Optional) Print Sample Output for Verification\n",
        "# -------------------------------\n",
        "for frame_file, ann_list in output_annotations.items():\n",
        "    print(\"Frame:\", frame_file)\n",
        "    for ann in ann_list:\n",
        "        print(ann)\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "YifMqUw4gIat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Cross-Camera & Temporal Matching"
      ],
      "metadata": {
        "id": "SbmxzIWfnCZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "# -------------------------------\n",
        "# Define Re-ID Model (ResNet50-based)\n",
        "# -------------------------------\n",
        "class ReIDModel(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(ReIDModel, self).__init__()\n",
        "        resnet = torchvision.models.resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]  # Remove the FC layer\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "        self.fc = nn.Linear(2048, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)  # shape: (B, 2048, 1, 1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "reid_model = ReIDModel(output_dim=512).to(device)\n",
        "reid_model.eval()\n",
        "\n",
        "# Define image transformation for Re-ID model\n",
        "transform = T.Compose([\n",
        "    T.Resize((256, 128)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def extract_feature_from_crop(crop_image):\n",
        "    \"\"\"\n",
        "    crop_image: a PIL image crop.\n",
        "    Returns: L2 normalized feature vector as a numpy array.\n",
        "    \"\"\"\n",
        "    img_tensor = transform(crop_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        feature = reid_model(img_tensor)\n",
        "    return feature.cpu().numpy().flatten()\n",
        "\n",
        "# -------------------------------\n",
        "# Assume `output_annotations` from previous steps is available.\n",
        "# Each annotation has a \"views\" list with \"bbox_2d\" computed.\n",
        "# Also assume a mapping from viewNum to the corresponding image folder.\n",
        "# -------------------------------\n",
        "base_images_dirs = {\n",
        "    0: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C1\",\n",
        "    1: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C2\",\n",
        "    2: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C3\",\n",
        "    3: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C4\",\n",
        "    4: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C5\",\n",
        "    5: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C6\",\n",
        "    6: \"/content/drive/MyDrive/Wildtrack/Image_subsets/C7\"\n",
        "}\n",
        "\n",
        "# Get list of frame annotation files (assumed to be keys in output_annotations)\n",
        "frame_files = sorted(output_annotations.keys())\n",
        "\n",
        "# -------------------------------\n",
        "# Cross-Camera & Temporal Matching\n",
        "# -------------------------------\n",
        "# We'll use a simple incremental matching strategy.\n",
        "# \"tracks\" is a dict mapping track_id to track info.\n",
        "tracks = {}  # track_id -> dict with keys: 'detections' (list), 'last_feature', 'last_frame', 'last_camera'\n",
        "next_track_id = 0\n",
        "similarity_threshold = 0.8\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    # Normalize and compute dot product\n",
        "    return np.dot(vec1 / np.linalg.norm(vec1), vec2 / np.linalg.norm(vec2))\n",
        "\n",
        "# Process each frame in temporal order.\n",
        "for frame_file in frame_files:\n",
        "    ann_list = output_annotations[frame_file]\n",
        "    # Derive image filename from frame_file (e.g., \"00001020.json\" -> \"00001020.jpg\")\n",
        "    image_filename = frame_file.replace(\".json\", \".jpg\")\n",
        "    for ann in ann_list:\n",
        "        for view in ann[\"views\"]:\n",
        "            bbox = view.get(\"bbox_2d\")\n",
        "            if bbox is None:\n",
        "                continue\n",
        "            view_num = view.get(\"viewNum\")\n",
        "            base_dir = base_images_dirs.get(view_num)\n",
        "            if base_dir is None:\n",
        "                continue\n",
        "            image_path = os.path.join(base_dir, image_filename)\n",
        "            if not os.path.exists(image_path):\n",
        "                continue\n",
        "            try:\n",
        "                pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "            except Exception as e:\n",
        "                continue\n",
        "            xmin, ymin, width, height = bbox\n",
        "            crop = pil_image.crop((xmin, ymin, xmin + width, ymin + height))\n",
        "\n",
        "            # Extract re-id feature\n",
        "            feature = extract_feature_from_crop(crop)\n",
        "\n",
        "            # Matching: compare with existing tracks' last feature\n",
        "            best_track_id = None\n",
        "            best_similarity = -1\n",
        "            for track_id, track_data in tracks.items():\n",
        "                sim = cosine_similarity(feature, track_data[\"last_feature\"])\n",
        "                if sim > best_similarity:\n",
        "                    best_similarity = sim\n",
        "                    best_track_id = track_id\n",
        "            if best_similarity >= similarity_threshold:\n",
        "                # Assign existing track\n",
        "                tracks[best_track_id][\"detections\"].append({\n",
        "                    \"frame\": frame_file,\n",
        "                    \"viewNum\": view_num,\n",
        "                    \"bbox\": bbox,\n",
        "                    \"feature\": feature.tolist()\n",
        "                })\n",
        "                tracks[best_track_id][\"last_feature\"] = feature\n",
        "                tracks[best_track_id][\"last_frame\"] = frame_file\n",
        "                tracks[best_track_id][\"last_camera\"] = view_num\n",
        "                view[\"track_id\"] = best_track_id\n",
        "            else:\n",
        "                # Create a new track\n",
        "                new_track_id = next_track_id\n",
        "                next_track_id += 1\n",
        "                tracks[new_track_id] = {\n",
        "                    \"detections\": [{\n",
        "                        \"frame\": frame_file,\n",
        "                        \"viewNum\": view_num,\n",
        "                        \"bbox\": bbox,\n",
        "                        \"feature\": feature.tolist()\n",
        "                    }],\n",
        "                    \"last_feature\": feature,\n",
        "                    \"last_frame\": frame_file,\n",
        "                    \"last_camera\": view_num\n",
        "                }\n",
        "                view[\"track_id\"] = new_track_id\n",
        "\n",
        "# -------------------------------\n",
        "# Output Matching Results\n",
        "# -------------------------------\n",
        "# Print the assembled tracks, including last seen information.\n",
        "for track_id, track_data in tracks.items():\n",
        "    print(f\"Track ID: {track_id}\")\n",
        "    print(f\"Last seen in frame: {track_data['last_frame']} at camera: {track_data['last_camera']}\")\n",
        "    print(\"Detections:\")\n",
        "    for d in track_data[\"detections\"]:\n",
        "        print(d)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "6v6v9pgKknjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CRKCYXmynNdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}